{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "- To determine the emotional affect of text.\n",
    "- Generally done very poorly.\n",
    "- Little agreement on sentiment.\n",
    "- Lots of amazing machine learning performance scores.\n",
    "- Works poorly with software engineering.\n",
    "- Beneath the Tip of the Iceberg: Current Challenges and New Directions in Sentiment Analysis Research https://arxiv.org/pdf/2005.00357.pdf\n",
    "\n",
    "# LIWC\n",
    "\n",
    "- Word Lists of different categories\n",
    "- https://lit.eecs.umich.edu/geoliwc/liwc_dictionary.html\n",
    "- Not just positive and negative.\n",
    "- Pennebaker, J.W., & Francis, M.E. (1996). Cognitive, emotional, and language processes in disclosure. Cognition and Emotion, 10, 601-626.\n",
    "- SE Paper: P. C. Rigby and A. E. Hassan, \"What Can OSS Mailing Lists Tell Us? A Preliminary Psychometric Text Analysis of the Apache Developer Mailing List,\" Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007), Minneapolis, MN, 2007, pp. 23-23, doi: 10.1109/MSR.2007.35. https://ieeexplore.ieee.org/document/4228660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Modern\" Sentiment Analysis\n",
    "\n",
    "- Instead of being more descriptive modern senitment analysis has made it easy for computers rather than useful for people.\n",
    "- Typically 2 to 3 labels: Positive, Neutral, Negative, or just Positive and Negative\n",
    "- Ignores context\n",
    "- Simple classifiers.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/hindle1/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hindle1/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Portion based on\n",
    "# Based on https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "# By By Shaumik Daityari, September 26, 2019\n",
    "#\n",
    "# Run this if you don't have twitter_samples\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "# Run this if you don't have vader_lexicon\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Get data\n",
    "\n",
    "- Here's some non-SE data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', '@97sides CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days', '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM', \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\", '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.', 'Jgh , but we have to go to Bayan :D bye', 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']\n",
      "['hopeless for tmr :(', \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\", '@Hegelbon That heart sliding into the waste basket. :(', '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too', 'Dang starting next week I have \"work\" :(', \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\", '@RileyMcDonough make me smile :((', '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln', 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"', 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweets[0:10])\n",
    "print(negative_tweets[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess -- remove stop words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "positives = preprocess_documents( positive_tweets )\n",
    "negatives = preprocess_documents( negative_tweets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['followfridai', 'franc', 'int', 'pkuchli', 'milipol', 'pari', 'engag', 'member', 'commun', 'week'], ['lambja', 'hei', 'jame', 'odd', 'contact', 'centr', 'abl', 'assist', 'thank'], ['despiteoffici', 'listen', 'night', 'bleed', 'amaz', 'track', 'scotland'], ['side', 'congrat'], ['yeaaaah', 'yippppi', 'accnt', 'verifi', 'rqst', 'succe', 'got', 'blue', 'tick', 'mark', 'profil', 'dai'], ['bhaktisbant', 'pallaviruhail', 'irresist', 'flipkartfashionfridai', 'http', 'ebzlvenm'], ['like', 'love', 'custom', 'wait', 'long', 'hope', 'enjoi', 'happi', 'fridai', 'lwwf', 'http', 'smyyriipxi'], ['impatientraid', 'second', 'thought', 'there’', 'time', 'new', 'short', 'enter', 'sheep', 'bui'], ['jgh', 'bayan', 'bye'], ['act', 'mischiev', 'call', 'etl', 'layer', 'hous', 'wareh', 'app', 'katamari', 'well…', 'impli']]\n",
      "[['hopeless', 'tmr'], ['kid', 'section', 'ikea', 'cute', 'shame', 'nearli', 'month'], ['hegelbon', 'heart', 'slide', 'wast', 'basket'], ['ketchburn', 'hate', 'japanes', 'bani'], ['dang', 'start', 'week', 'work'], ['god', 'babi', 'face', 'http', 'fcwgvaki'], ['rileymcdonough', 'smile'], ['fggstar', 'stuartthul', 'work', 'neighbour', 'motor', 'ask', 'said', 'hate', 'updat', 'search', 'http', 'xvmtuikwln'], ['tahuodyi', 'sialan', 'http', 'hvixcrl'], ['athabasca', 'glacier', 'athabasca', 'glacier', 'jasper', 'jaspernationalpark', 'alberta', 'explorealberta', 'http', 'dzzdqmfcz']]\n"
     ]
    }
   ],
   "source": [
    "print(positives[0:10])\n",
    "print(negatives[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce vocabulary and move to bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "words = Dictionary(positives + negatives)\n",
    "MAXWORDS=30000\n",
    "words.filter_extremes(no_below=5, no_above=0.5, keep_n=MAXWORDS)\n",
    "positive_docs = [words.doc2bow(doc) for doc in positives]\n",
    "negative_docs = [words.doc2bow(doc) for doc in negatives]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(13, 1), (14, 1), (15, 1), (16, 1)], [(17, 1)], [(18, 1), (19, 1), (20, 1), (21, 1)], [(22, 1), (23, 1), (24, 1), (25, 1)], [(24, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)], [(35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)], [(42, 1), (43, 1)], [(44, 1), (45, 1), (46, 1), (47, 1)]]\n",
      "[[], [(152, 1), (180, 1), (619, 1), (630, 1), (1204, 1), (1298, 1)], [(554, 1), (712, 1)], [(545, 1), (703, 1)], [(6, 1), (73, 1), (259, 1)], [(24, 1), (179, 1), (181, 1), (1255, 1)], [(302, 1)], [(24, 1), (73, 1), (475, 1), (545, 1), (751, 1), (830, 1)], [(24, 1)], [(24, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(positive_docs[0:10])\n",
    "print(negative_docs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "X = [sparse2full(x,length=MAXWORDS) for x in positive_docs + negative_docs]\n",
    "y = [\"P\" for x in positive_docs] + \\\n",
    "    [\"N\" for x in negative_docs]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size=0.33)\n",
    "\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_hat_train = nb.predict(X_train)\n",
    "y_hat_test  = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2337  996]\n",
      " [ 535 2832]]\n",
      "0.7872133425990271\n",
      "[[1105  562]\n",
      " [ 354 1279]]\n",
      "0.7363270005757053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "print(confusion_matrix(y_train, y_hat_train))\n",
    "print(f1_score(y_train, y_hat_train, pos_label=\"P\"))\n",
    "print(confusion_matrix(y_test, y_hat_test))\n",
    "print(f1_score(y_test, y_hat_test, pos_label=\"P\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's a rule based learn called Vader\n",
    "\n",
    "http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
    "Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
    "Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.7579}, {'neg': 0.145, 'neu': 0.585, 'pos': 0.27, 'compound': 0.6229}, {'neg': 0.0, 'neu': 0.706, 'pos': 0.294, 'compound': 0.7959}, {'neg': 0.0, 'neu': 0.123, 'pos': 0.877, 'compound': 0.7983}, {'neg': 0.0, 'neu': 0.718, 'pos': 0.282, 'compound': 0.795}, {'neg': 0.0, 'neu': 0.565, 'pos': 0.435, 'compound': 0.6597}, {'neg': 0.063, 'neu': 0.417, 'pos': 0.52, 'compound': 0.9466}, {'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compound': 0.4588}, {'neg': 0.0, 'neu': 0.619, 'pos': 0.381, 'compound': 0.7615}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "0.8531605113636365\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "scores = [sia.polarity_scores(x) for x in positive_tweets + negative_tweets]\n",
    "print(scores[0:10])\n",
    "pn_scores = [\"N\" if x['neg'] > x['pos'] else \"P\" for x in scores]\n",
    "print(f1_score(y, pn_scores,pos_label=\"P\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Ahmed, T. , Bosu, A., Iqbal, A. and Rahimi, S. 's README for SentiCR\n",
    "\n",
    "https://github.com/senticr/SentiCR\n",
    "\n",
    "Or my python3 version \n",
    "\n",
    "https://github.com/abramhindle/SentiCR\n",
    "\n",
    "\n",
    "# SentiCR\n",
    "\n",
    "SentiCR is an automated sentiment analysis tool for code review comments. SentiCR uses supervised learning algorithms to train\n",
    "models based on 1600 manually label code review comments (https://github.com/senticr/SentiCR/blob/master/SentiCR/oracle.xlsx). Features of SentiCR include:\n",
    "\n",
    "- Special preprocessing steps to exclude URLs and code snippets\n",
    "- Special preprocessing for emoticons\n",
    "- Preprocessing steps for contractions\n",
    "- Special handling of negation phrases through precise identification\n",
    "- Optimized for the SE domain\n",
    "\n",
    "## Performance\n",
    "In our hundred ten-fold cross-validations, SentiCR achieved 83.03% accuracy (i.e., human level accuracy), 67.84% precision,\n",
    "58.35% recall, and 0.62 f-score on a Gradient Boosting Tree based model. Details cross validation results are included here:\n",
    "https://github.com/senticr/SentiCR/tree/master/cross-validation-results\n",
    "\n",
    "## Cite\n",
    "\n",
    "Ahmed, T. , Bosu, A., Iqbal, A. and Rahimi, S., \"SentiCR: A Customized Sentiment Analysis Tool for Code Review Interactions\", In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering (NIER track).\n",
    "\n",
    "```\n",
    "@INPROCEEDINGS{Ahmed-et-al-SentiCR,\n",
    "   author = {Ahmed, Toufique and Bosu, Amiangshu and Iqbal, Anindya and Rahimi, Shahram},\n",
    "   title = {{SentiCR: A Customized Sentiment Analysis Tool for Code Review Interactions}},\n",
    "   year = {2017},\n",
    "   series = {ASE '17},\n",
    "   booktitle = {32nd IEEE/ACM International Conference on Automated Software Engineering (NIER track)},\n",
    "}\n",
    "```\n",
    "\n",
    "http://amiangshu.com/papers/senticr-ase.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from oracle..\n",
      "Training classifier model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hindle1/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ani', 'continu', 'deleg', 'doe', 'doubl', 'dure', 'els', 'endwhil', 'extend', 'implement', 'includ', 'interfac', 'namespac', 'nativ', 'nowwhil', 'onc', 'ourselv', 'overrid', 'packag', 'privat', 'protect', 'rais', 'readon', 'requir', 'sign', 'synchron', 'themselv', 'tri', 'veri', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure I entirely understand what you are saying. However, looking at file_linux_test.go I'm pretty sure an interface type would be easier for people to use.\n",
      " Score: [-1.]\n",
      "I think it always returns it as 0.\n",
      " Score: [0.]\n",
      "If the steal does not commit, there's no need to clean up _p_'s runq. If it doesn't commit, runqsteal just won't update runqtail, so it won't matter what's in _p_.runq.\n",
      " Score: [-1.]\n",
      "Please change the subject: s:internal/syscall/windows:internal/syscall/windows/registry:\n",
      " Score: [0.]\n",
      "I don't think the name Sockaddr is a good choice here, since it means something very different in the C world.  What do you think of SocketConnAddr instead?\n",
      " Score: [-1.]\n",
      "could we use sed here?  https://go-review.googlesource.com/#/c/10112/1/src/syscall/mkall.sh  it will make the location of the build tag consistent across files (always before the package statement).\n",
      " Score: [0.]\n",
      "Is the implementation hiding here important? This would be simpler still as:  typedef struct GoSeq {   uint8_t *buf;   size_t off;   size_t len;   size_t cap; } GoSeq;\n",
      " Score: [0.]\n",
      "Make sure you test both ways, or a bug that made it always return false would cause the test to pass.  assertTrue(Testpkg.Negate(false));  assertFalse(Testpkg.Negate(true)); + If you want to use the assertEquals form, be sure the message makes clear what actually happened and what was expected (e.g. Negate(true) != false). \n",
      " Score: [0.]\n"
     ]
    }
   ],
   "source": [
    "from SentiCR import  SentiCR\n",
    "\n",
    "sentiment_analyzer=SentiCR()\n",
    "senticr = sentiment_analyzer\n",
    "#All examples are acutal code review comments from Go lang\n",
    "\n",
    "sentences=[\"I'm not sure I entirely understand what you are saying. \"+\\\n",
    "           \"However, looking at file_linux_test.go I'm pretty sure an interface type would be easier for people to use.\",\n",
    "           \"I think it always returns it as 0.\",\n",
    "           \"If the steal does not commit, there's no need to clean up _p_'s runq. If it doesn't commit,\"+\\\n",
    "             \" runqsteal just won't update runqtail, so it won't matter what's in _p_.runq.\",\n",
    "           \"Please change the subject: s:internal/syscall/windows:internal/syscall/windows/registry:\",\n",
    "           \"I don't think the name Sockaddr is a good choice here, since it means something very different in \"+\\\n",
    "           \"the C world.  What do you think of SocketConnAddr instead?\",\n",
    "           \"could we use sed here? \"+\\\n",
    "            \" https://go-review.googlesource.com/#/c/10112/1/src/syscall/mkall.sh \"+\\\n",
    "            \" it will make the location of the build tag consistent across files (always before the package statement).\",\n",
    "           \"Is the implementation hiding here important? This would be simpler still as: \"+\\\n",
    "          \" typedef struct GoSeq {   uint8_t *buf;   size_t off;   size_t len;   size_t cap; } GoSeq;\",\n",
    "           \"Make sure you test both ways, or a bug that made it always return false would cause the test to pass. \"+\\\n",
    "        \" assertTrue(Testpkg.Negate(false)); \"+\\\n",
    "        \" assertFalse(Testpkg.Negate(true)); +\"\\\n",
    "        \" If you want to use the assertEquals form, be sure the message makes clear what actually happened and \" +\\\n",
    "        \"what was expected (e.g. Negate(true) != false). \"]\n",
    "\n",
    "for sent in sentences:\n",
    "    score=sentiment_analyzer.get_sentiment_polarity(sent)\n",
    "    print(sent+\"\\n Score: \"+str(score))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.])]\n",
      "0.6959335866047559\n"
     ]
    }
   ],
   "source": [
    "scores = [sentiment_analyzer.get_sentiment_polarity(x) for x in positive_tweets + negative_tweets]\n",
    "print(scores[0:10])\n",
    "pn_scores = [\"N\" if x < 0 else \"P\" for x in scores]\n",
    "print(f1_score(y, pn_scores,pos_label=\"P\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest deep learning transformer based sentiment analysis\n",
    "\n",
    "- Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Alexander Wong\n",
    "# using transformers to sentiment analysis\n",
    "import nltk\n",
    "from  transformers import pipeline\n",
    "sentence_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "tokenizer = TweetTokenizer()\n",
    "N = 600\n",
    "y_hug = [\"P\" for x in range(N)] + [\"N\" for x in range(N)]\n",
    "sentences = [sentence_tokenizer.tokenize(x)[0] for x in positive_tweets[0:N] + negative_tweets[0:N]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through the sentiment analyzer to get pos/neg label and score\n",
    "sentiment_batches = [sentiment_analyzer(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9260645508766174}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_batches[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9260645508766174},\n",
       " {'label': 'POSITIVE', 'score': 0.9929360747337341},\n",
       " {'label': 'POSITIVE', 'score': 0.9997010827064514},\n",
       " {'label': 'POSITIVE', 'score': 0.9924265146255493},\n",
       " {'label': 'NEGATIVE', 'score': 0.9931758046150208},\n",
       " {'label': 'POSITIVE', 'score': 0.9995020627975464},\n",
       " {'label': 'NEGATIVE', 'score': 0.9852420091629028},\n",
       " {'label': 'NEGATIVE', 'score': 0.9981480836868286},\n",
       " {'label': 'POSITIVE', 'score': 0.9309679865837097},\n",
       " {'label': 'POSITIVE', 'score': 0.9934095144271851}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = [sentence for batch in sentiment_batches for sentence in batch]\n",
    "sentiments[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6245353159851301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "y_hat = [\"N\" if x['label'] == 'NEGATIVE' else 'P' for x in sentiments]\n",
    "print(f1_score(y_hug, y_hat,pos_label=\"P\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From \n",
    "Pletea D, Vasilescu B, Serebrenik A. \n",
    "Security and emotion: sentiment analysis of security discussions on GitHub. \n",
    "InProceedings of the 11th working conference on mining software repositories \n",
    "2014 May 31 (pp. 348-351).\n",
    "\"\"\"\n",
    "keywords = \"\"\"access policy, access role, access-policy, access-role, accesspolicy, accessrole, aes, audit, authentic, authority, au-\n",
    "thoriz, biometric, black list, black-list, blacklist, blacklist,\n",
    "cbc, certificate, checksum, cipher, clearance, confidentiality,\n",
    "cookie, crc, credential, crypt, csrf, decode, defensive programming,\n",
    "defensive-programming, delegation, denial of service,\n",
    "denial-of-service, diffie-hellman, dmz, dotfuscator, dsa,\n",
    "ecdsa, encode, escrow, exploit, firewall, forge, forgery, gss\n",
    "api, gss-api, gssapi, hack, hash, hmac, honey pot, honeypot, \n",
    "honeypot, inject, integrity, kerberos, ldap, login, malware,\n",
    "md5, nonce, nss, oauth, obfuscat, open auth, openauth,\n",
    "openauth, openid, owasp, password, pbkdf2, pgp, phishing, pki, privacy, private key, private-key, privatekey, privilege, \n",
    "public key, public-key, publickey, rbac, rc4, repudiation,\n",
    "rfc 2898, rfc-2898, rfc2898, rijndael, rootkit, rsa, salt, saml,\n",
    "sanitiz, secur, sha, shell code, shell-code, shellcode, shibboleth, \n",
    "signature, signed, signing, sing sign-on, single signon,\n",
    "single-sign-on, smart assembly, smart-assembly, smartassembly, \n",
    "snif, spam, spnego, spoofing, spyware, ssl, sso,\n",
    "steganography, tampering, trojan, trust, violat, virus, white\n",
    "list, white-list, whitelist, x509, xss.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9758636355400085}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analyzer(sentence_tokenizer.tokenize(keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from oracle..\n",
      "Training classifier model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hindle1/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ani', 'continu', 'deleg', 'doe', 'doubl', 'dure', 'els', 'endwhil', 'extend', 'implement', 'includ', 'interfac', 'namespac', 'nativ', 'nowwhil', 'onc', 'ourselv', 'overrid', 'packag', 'privat', 'protect', 'rais', 'readon', 'requir', 'sign', 'synchron', 'themselv', 'tri', 'veri', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(SentiCR().get_sentiment_polarity(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.025, 'neu': 0.887, 'pos': 0.087, 'compound': 0.8225}\n"
     ]
    }
   ],
   "source": [
    "print(sia.polarity_scores(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_doc = sparse2full(words.doc2bow(preprocess_documents(keywords)[0]),length=MAXWORDS)\n",
    "keyword_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P'], dtype='<U1')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict([keyword_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P'], dtype='<U1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"I forgot to charge my phone over night\"\n",
    "phrase = \"The sky is blue\"\n",
    "phrase = \"Stuff is not working out for me today\"\n",
    "phrase = \"I don't like pizza\"\n",
    "custom_doc = sparse2full(words.doc2bow(preprocess_documents(phrase)[0]),length=MAXWORDS)\n",
    "nb.predict([custom_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
