#+TITLE:     Inter-rater Reliability
#+AUTHOR:    Abram Hindle
#+EMAIL:     abram.hindle@ualberta.ca
#+DATE:      2012-04-27 Tue
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  en
#+PROPERTY: header-args:R :session RIRR :results output
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 

#+startup: oddeven

#+startup: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+latex_header: \mode<beamer>{\usetheme{Madrid}}
#+BEAMER_FRAME_LEVEL: 2

#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)

* Introduction
** Imagine
   - We're tagging or rating items
     - commits
     - topics
     - bug reports
   - We're using multiple people to tag or rate these items
   - How do we ensure that we have agreement
** Example
   - "I made a change to the install documentation."
     - Is this a portability relevant commit message?
   - "I made a change to the install documentation regarding OSX."
     - Is this a portability relevant commit message?
   - "I made a change to the install documentation regarding OSX,
     Windows and Linux."
     - Is this a portability relevant commit message?
** We can disagree
   - With ratings this is a problem
   - Especially if we want to model or predict ratings
   - How random are the ratings?
   - Is there a bias?
** Reliability
   - So how reliability are ratings.
   - Is there agreement?
   - What does lack of agreement mean?
* Inter-rater reliability
** How
   - We can measure correlation
     - Not really meant for it, but a good start
     -  Pearson
     - Spearman
   -  Cohen's Kappa Statistic 
     - Good for 2 raters
   -  Fleiss's Kappa
     - Good for more
** R
   - Perason  cor(x,y,method="pearson")
   - Spearman cor(x,y,method="spearman")
   - library(irr)
   - columns of a matrix
   - kappa2(yourMatrix) 
   - kappm.fleiss(yourMatrix)
** R
   - Go and look at the code
** What's a good Kappa or Correlation?
   - 1 is good
   - 0 or worse is bad
   - 0.1 will get reviewers annoyed
** Pay attention to
   - num raters
   - num subjects
   - Kappa 
   - p-value
** Threats to IRR
   - Kappa of all same rank is NaN
   - Lack of range in ratings 0,1 versus 1-5
   - Class imbalance, if 10\% is rated one way this can cause a problem

* Interrater Reliability Analysis in R

This tutorial demonstrates how to compute interrater reliability using R, specifically focusing on Cohen's kappa and Fleiss' kappa. 

* Loading Required Libraries and Data

First, we need to load the necessary libraries and the datasets containing our annotations.

#+BEGIN_SRC R
# Load required library
library(irr)

# Set cores for parallel processing
cores <- 8

# Load the data
pgsqln <- read.csv("../IRR/output/pgsqln.arff.csv", header=TRUE)
pgsqla <- read.csv("../IRR/output/pgsqla.arff.csv", header=TRUE)

# Select relevant columns
nn <- length(pgsqln)
na <- length(pgsqla)
pn <- pgsqln[, (nn-6):nn]
pa <- pgsqla[, (na-6):na]
head(pa)
head(pn)
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: lpSolve
  A_portability A_functionality A_reliability A_maintainability A_efficiency A_usability A_none
1             1               0             1                 0            1           0      0
2             1               0             1                 0            0           0      0
3             1               0             1                 1            0           0      0
4             0               1             1                 1            0           0      0
5             1               0             0                 0            1           0      0
6             0               1             0                 1            0           1      0
  A_functionality A_portability A_usability A_reliability A_efficiency A_maintainability A_none
1               0             1           0             0            0                 0      0
2               0             1           0             0            0                 0      0
3               0             0           0             1            0                 0      0
4               0             0           0             1            0                 1      0
5               0             0           0             0            1                 0      0
6               0             0           0             0            0                 0      1
#+end_example

* Defining Annotations

Next, we define the annotations (ratings) for the different students represented in our dataset.

#+BEGIN_SRC R
# Define student ratings
students <- data.frame(
  abr = c(1,1,0,0,0,0),
  kal = c(0,1,0,0,0,0),
  sam = c(0,1,1,0,0,0),
  reb = c(0,1,0,0,0,0),
  lui = c(0,1,1,0,1,0),
  art = c(0,1,1,0,0,0)
)
#+END_SRC

#+RESULTS:

* Krippendorff's Alpha

#+begin_src R
# Load the required package for calculating Krippendorff's alpha
library(irr)

# Cohen's Kappa for each pair of students
kappa_results <- combn(ncol(students), 2, function(x) {
  kappa <- kappa2(as.matrix(students[, x]))
  return(list(pair = colnames(students)[x], kappa = kappa$value))
}, simplify = FALSE)

# Display Cohen's Kappa results
kappa_results <- do.call(rbind, lapply(kappa_results, function(x) data.frame(pair = paste(x$pair, collapse = " vs "), kappa = x$kappa)))
print(kappa_results)

# Krippendorff's alpha
kripp.alpha(as.matrix(students), method = "nominal")
#print(paste("Krippendorff's Alpha:", kripp_alpha))
#+end_src

#+RESULTS:
#+begin_example
         pair     kappa
1  abr vs kal 0.5714286
2  abr vs sam 0.2500000
3  abr vs reb 0.5714286
4  abr vs lui 0.0000000
5  abr vs art 0.2500000
6  kal vs sam 0.5714286
7  kal vs reb 1.0000000
8  kal vs lui 0.3333333
9  kal vs art 0.5714286
10 sam vs reb 0.5714286
11 sam vs lui 0.6666667
12 sam vs art 1.0000000
13 reb vs lui 0.3333333
14 reb vs art 0.5714286
15 lui vs art 0.6666667
 Krippendorff's alpha

 Subjects = 6 
   Raters = 6 
    alpha = -0.12
#+end_example

This code calculates Cohen's kappa for each pair of student ratings and computes Krippendorff's alpha for the entire rating set.

* Computing Cohen's Kappa

We compute Cohen's kappa for different scenarios to assess agreement between two raters.

#+BEGIN_SRC R
head(matrix(c(c(1:100)*0,c(1:100)*0+1),ncol=2))
#+END_SRC

#+RESULTS:
:      [,1] [,2]
: [1,]    0    1
: [2,]    0    1
: [3,]    0    1
: [4,]    0    1
: [5,]    0    1
: [6,]    0    1

#+BEGIN_SRC R
# Example kappa calculations
head(  matrix(c(c(1:100)*0,c(1:100)*0+1),ncol=2))
print("All disagreement")
kappa2(matrix(c(c(1:100)*0,c(1:100)*0+1),ncol=2))
print("All agreement")
head(matrix(c(c(1:100)*0+1,c(1:100)*0+1),ncol=2))
print("Rare agreement on minority class")
head(matrix(round(c( 1*(runif(600)>.9), 1*(runif(600)>.9))), ncol=2))
kappa2(matrix(round(c( 1*(runif(600)>.9)+1, 1*(runif(600)>.9)+1)), ncol=2))
print("Lots of negative agreement but rare agreement on minority class")
kappa2(matrix(c( 1*(runif(600)>.99), 1*(runif(600)>.99)), ncol=2))
print("Lots of negative agreement but lots of correlated agreement on 1")
kappa2(matrix(c( 1*(runif(600)>.99),c(1:10)*0+1, 1*(runif(600)>.99),c(1:10)*0+1), ncol=2))
print("Uncorrelated")
kappa2(matrix(c( (runif(600)>.5), (runif(600)>.5)), ncol=2))
kappa2(matrix(c( 1*(runif(600)>.1), 1*(runif(600)>.1)), ncol=2))
#+END_SRC

#+RESULTS:
#+begin_example
     [,1] [,2]
[1,]    0    1
[2,]    0    1
[3,]    0    1
[4,]    0    1
[5,]    0    1
[6,]    0    1
[1] "All disagreement"
 Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 100 
   Raters = 2 
    Kappa = 0 

        z = NaN 
  p-value = NaN
[1] "All agreement"
     [,1] [,2]
[1,]    1    1
[2,]    1    1
[3,]    1    1
[4,]    1    1
[5,]    1    1
[6,]    1    1
[1] "Rare agreement on minority class"
     [,1] [,2]
[1,]    0    0
[2,]    0    1
[3,]    1    0
[4,]    0    0
[5,]    0    0
[6,]    0    0
 Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 600 
   Raters = 2 
    Kappa = -0.0384 

        z = -0.949 
  p-value = 0.342
[1] "Lots of negative agreement but rare agreement on minority class"
 Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 600 
   Raters = 2 
    Kappa = -0.0139 

        z = -0.346 
  p-value = 0.729
[1] "Lots of negative agreement but lots of correlated agreement on 1"
 Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 610 
   Raters = 2 
    Kappa = 0.615 

        z = 15.5 
  p-value = 0
[1] "Uncorrelated"
 Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 600 
   Raters = 2 
    Kappa = 0.0477 

        z = 1.17 
  p-value = 0.241
 Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 600 
   Raters = 2 
    Kappa = 0.0571 

        z = 1.4 
  p-value = 0.162
#+end_example

* Computing Fleiss' Kappa

Next, we will calculate Fleiss' kappa, which is suitable for assessing the reliability of multiple raters.

#+BEGIN_SRC R
# Fleiss' Kappa calculations
fleiss_results <- sapply(names(pa), function(i) {
  mat <- matrix(c(as.factor(pa[, i]), as.factor(pn[, i])), ncol=2)
  kappam.fleiss(mat)$value
})
fleiss_results
#+END_SRC

#+RESULTS:
:     A_portability   A_functionality     A_reliability A_maintainability      A_efficiency       A_usability            A_none 
:      -0.046627578      -0.043241020       0.004101225       0.081275504       0.211635362      -0.078431373       0.054121959

* Correlation Analysis

Perform correlation analysis between the raters to assess their agreement quantitatively.

#+BEGIN_SRC R
# Correlation between raters
cor_results <- sapply(names(pa), function(i) {
  cor(pa[, i], pn[, i], method="spearman")
})
cor_results
#+END_SRC

#+RESULTS:
:     A_portability   A_functionality     A_reliability A_maintainability      A_efficiency       A_usability            A_none 
:       0.253037469      -0.014415214       0.004579664       0.082215613       0.258037216       0.014436230       0.080667468

* Krippendorff's Alpha

The rows are supposed to be the raters. The columns are subjects.

#+BEGIN_SRC R
# Correlation between raters
kr_results <- sapply(names(pa), function(i) {
  c(kripp.alpha(t(matrix(c(pa[,i],pn[,i]),ncol=2))),cor(pa[,i],pn[,i]))
})
kr_results
#+END_SRC

#+RESULTS:
#+begin_example
            A_portability          A_functionality        A_reliability          A_maintainability      A_efficiency          
method      "Krippendorff's alpha" "Krippendorff's alpha" "Krippendorff's alpha" "Krippendorff's alpha" "Krippendorff's alpha"
subjects    640                    640                    640                    640                    640                   
raters      2                      2                      2                      2                      2                     
irr.name    "alpha"                "alpha"                "alpha"                "alpha"                "alpha"               
value       -0.0458099             -0.04242599            0.004879271            0.08199326             0.2122513             
stat.name   "nil"                  "nil"                  "nil"                  "nil"                  "nil"                 
statistic   NULL                   NULL                   NULL                   NULL                   NULL                  
cm          numeric,4              numeric,4              numeric,4              numeric,4              numeric,4             
data.values character,2            character,2            character,2            character,2            character,2           
nmatchval   1280                   1280                   1280                   1280                   1280                  
data.level  "nominal"              "nominal"              "nominal"              "nominal"              "nominal"             
            0.2530375              -0.01441521            0.004579664            0.08221561             0.2580372             
            A_usability            A_none                
method      "Krippendorff's alpha" "Krippendorff's alpha"
subjects    640                    640                   
raters      2                      2                     
irr.name    "alpha"                "alpha"               
value       -0.07758885            0.05486093            
stat.name   "nil"                  "nil"                 
statistic   NULL                   NULL                  
cm          numeric,4              numeric,4             
data.values character,2            character,2           
nmatchval   1280                   1280                  
data.level  "nominal"              "nominal"             
            0.01443623             0.08066747
#+end_example
