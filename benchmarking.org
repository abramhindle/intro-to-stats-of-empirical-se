* Performance Benchmarking
  - Abram Hindle <hindle1@ualberta.ca> <2020-12-02 Wed>
  - Discussing SOFTWARE Benchmarking
** Purpose
   - to know how fast hardware is
   - to know how fast software is
   - to know how a system handles various loads
   - to push a system into a state to observe effects
   - to compare systems based on performance
** Types of Performance Benchmarks
   - Throughput
     - how many operations per time unit
       - floating point ops per second
   - Time-based / Task-based
     - the time or resource consumed to complete a task
   - Mixed
     - a combination
** General Methodology
   - Setup System under test
   - repeat 10+ times or
     - setup test
     - Test & Measure
     - tear down test
     - record result
   - summarize and compare
*** Choices
    - what to measure
    - how to measure
    - how to setup
    - how to teardown
    - what to control for
    - what not to control for
    - realism concerns
** SPEC
   - Standard Performance Evaluation Corporation
   - Maintains and charges money for benchmarks.
   - Because they charge money for benchmarks that include free
     software I would remain skeptical.
   - Academics do man the committees that produce the benchmarks.
** CPU
   - Has CPU state which is the frequency that the CPU runs at.
   - lower frequency is slower and uses less energy, higher latency
   - high frequency is faster and uses more energy.
   - General purpose CPUs do a lot of things at once
*** Integer
    - How does it move data around and do basic math
*** Floating Point
    - how does it deal with floating point numbers
*** Vectorized Performance
    - Batching similar results
    - mapping values
    - can impact performance with compilers and JIT that support or don't support vector ops.
*** CPU States
    - Different frequencies consume different energy
    - Different frequencies on different cores    
*** Power issues
    - If there's not enough power the CPU might not run at appropriate rates and might clock down
    - If there's not enough voltage you can't use lots of different features
*** Heat issues
    - CPUs that get too hot will shutdown or clock down.
    - Heat can be inconsistent
    - Ambient enviroment matters
** SUT
   - System under test
   - what you are measuring
   - usually should be setup to represent the scenario you are testing
   - should be controllable
   - can be a simple piece of software
   - can be a cloud
** Confounds
*** Cache
    - caching at all levels will change performance
    - this is why people like to throw away initial measurements
      because they want the most cached highest performance examples
      - totally unrealistic though
*** Granularity of measurement
    - things on computers are quick
      - will you be able to capture the effect?
    - Did you measure the entire system or part of it
      - measuring part ignores the induced load
      - measuring everything hides effects in noise
*** Contexts
    - Realism
      - a phone in a pocket
      - a laptop in a cafe
      - a server in a datacenter
      - a cloud VM
    - Micro Benchmark
      - Not realistic
      - meant to know the best performance of a SUT
      - commonly used to compare 2 pieces of code
      - https://benchmarksgame-team.pages.debian.net/benchmarksgame/which-programs-are-fastest.html
      - Typically 300X improvement in a Micro Benchmark might show up
        as less than 5% performance improvement in the wild where the
        software is not used in the same way or at the same rate.
    - Scenarios
      - maybe an attempt at realism
      - but goal oriented
    - Task based
      - measure effectiveness at a task according to some resource limitations
*** Instrumentation overhead
    - how is the test being run? 
    - how much does it cost to the run the test
    - For UI tests
      - does it need to reflect on objects to know what to click
      - does it need DOM access?
      - is a click the same as a mushy finger click
      - injected touch events are less work that multitouch events from the touch screen
    - If you're watching method calls what is the cost of that?
*** Background Processes
    - If too much is going on measurements will be more variable
*** CPU State   
    - what state did you state the test in
    - is the governor on
    - what is the frequency
    - will it change frequency during the test
*** Temperature
    - What is the temperature of the SUT before and during test?
*** Network
    - If you're using wifi or the internet are the contexts comparable
      - Was amazon down?
      - Was it peak time of day?
    - If you run your tests in a VM or Docker you have overhead
*** Disk
    - Files get cached if you access them often
      - subsequent tests could be faster
    - You change the filesystems
    - repeatability of disk IO is not great
*** Containers
    - low overhead 
    - Copying on IO and Network
    - Good for CPU and memory bound jobs
    - Not clear about GPU
*** VMs
    - high overhead
    - quite controlled
    - repeatable
    - high IO overhead,
    - sometimes very high CPU and memory overhead
    - different virtualization strategies 
*** Cloud
    - usually you don't control the entire cloud
    - performance will differ based on what the cloud is being used for.
** Energy
*** Energy Star
    - https://www.energystar.gov/
    - Estimate energy consumption of appliances, including computers
*** SpecPower
    - Costs money. 3k
    - http://www.spec.org/power_ssj2008/
    - Methodology: http://www.spec.org/power/docs/SPEC-Power_and_Performance_Methodology.pdf
    - High workload benchmark primarily meant for servers.
    - Very server oriented
    - Ignores idle
    - Measures idle only as a baseline
    - Guidelines for power and temperature measure http://www.spec.org/power/docs/SPEC-Power_Measurement_Setup_Guide.pdf
*** ENERGISE
   - The Perils of Energy Mining: Measure a Bunch, Compare just Once
     https://softwareprocess.es/pubs/hindle2016D4SE-energymining.pdf
   - Environment -- prepare a stable testbed for energy measurement.
   - N-versions -- run a test across more than 1 version of the software.
   - Energy or power -- do we care about total energy consumed of a task or the per second
     cost of running a service?
   - Repeat -- 1 run is not enough, we need to run our tests multiple times to address
     background noise.
   - Granularity -- what level of measurement, how often, and what level of invasiveness
     of instrumentation?
   - Idle -- how do applications and service react with no load: is energy being wasted?
   - Statistics -- repeat measures call for summary statistics and comparing distributions.
   - Exceptions -- errors happen, how do we address with them or notice them?
*** GreenUp
   - SpeedUp measures for green software comparison
   - Paper:
     Abdulsalam S, Zong Z, Gu Q, Qiu M. Using the Greenup, Powerup, and
     Speedup metrics to evaluate software energy efficiency. In2015 Sixth
     International Green and Sustainable Computing Conference (IGSC) 2015
     Dec 14 (pp. 1-8). IEEE.
     - https://greensoft.cs.txstate.edu/wp-content/uploads/2018/05/Greenup_Powerup_Speedup.pdf
** Microbenchmarking
*** Python
    - timeit
    - cProfile
    - SnakeViz https://jiffyclub.github.io/snakeviz/
*** Java
    - JMH
    - https://openjdk.java.net/projects/code-tools/jmh/
      - Tutorial https://www.baeldung.com/java-microbenchmark-harness
** Evaluation
   - Evaluation of benchmarking is annoying
   - Reporting is typically done in a relative fashion and that is annoying because we can't see the measurements.
*** Reporting
    - Report the distribution (quartiles)
    - Use box plots! Or violin plots.
    - Provide absolute measurements
      - then provide relative measurements
        - such as speedup
    - Don't report ONLY means. Means aren't meaningful for many time based experiments.
      - Geometric mean is a lot safer
        - "How not to lie with Statistics: The Correct Way to Summarize Benchmark Results", Philip J. Fleming and John J. Wallace
          - http://ece.uprm.edu/~nayda/Courses/Icom5047F06/Papers/paper4.pdf
          - They argue you can't use MEAN on normalized results, you should use geometric mean.
            - They don't discuss median.
            - They do say: "However, it should be made clear that any measure of the mean value of data is misleading when there is
      - Geometric mean is the product of your N measurements taken to the Nth root.
        - e.g., 1,2,3 is (1*2*3)^3 = 1.8
large variance."
    - Provide details of the system including hardware
      - include dates
      - include versions of software
    - Report number of trials
    - COV is often used to compare variance
      - standard deviation / mean
      - it's a normalized variance
    - Some people report on the best results.
      - This baloney and deceptive. Don't do it.
      - There are some scenarios where this would matter but most cases this is dangerous.
    - You can use speedup but it's not very sound and makes assumptions that the mean is meaningful.
      - The Speedup-Test: A Statistical Methodology for Program Speedup Analysis and Computation Sid Touati, Julien Worms, SÃ©bastien Briais
        - https://hal.inria.fr/hal-00764454/document/
    - Effect sizes such as Cliff's Delta can be used to show that
      there is a difference and it has an effect.
*** Comparison
    - Use confidence intervals of the difference.
      - You should be comparing 10+ versus 10+ runs so get the 95%
        confidence interval of the difference! You can just boostrap
        it.
    - Use statistical tests to compare distributions
      - T-Test for normally distribution data
      - Wilcoxon / Mann-Whitney for not normal data.
    - Use paired tests when you are measuring/comparing a treatment of a test.
      - e.g., gcc -O2 versus gcc -O3
      - Wilcoxon signed rank test
      - The tests can estimate confidence intervals as well
    - When comparing rates/ratios consider if you can match the results as a poisson.
      - Consider the Exact Poisson tests with Matching Confidence Intervals
        - https://www.rdocumentation.org/packages/exactci/versions/1.3-3/topics/poisson.exact
      - Or Comparison of Poisson rates
    - When doing factor analysis consider:
      - Kruskall Wallace to determine if the factors matter
        - then pairwise (not paired) Wilcoxons or pairwise confidence intervals
*** If it is PASS/FAIL
    - use statistics for Beurnolli Trials
    - Like Two Proportions test.
    - Or if it is multicategorical X^2.
** Conferences:
   - ICPE - Performance Engineering
   - USENIX - has many performance parts to it
** References
   - The Perils of Energy Mining: Measure a Bunch, Compare just Once
     https://softwareprocess.es/pubs/hindle2016D4SE-energymining.pdf
   - SpecPOWER
    - http://www.spec.org/power_ssj2008/
   - EnergyStar
    - https://www.energystar.gov/
   - Wikipedia Benchmarks
     https://en.wikipedia.org/wiki/Benchmark_(computing)#Common_benchmarks
   - FLOSS Benchmarks
      https://en.wikipedia.org/wiki/Benchmark_(computing)#Open_source_benchmarks
   - Small Guide to Software Benchmarking by Markus PÃ¼schel
     - https://users.ece.cmu.edu/~pueschel/teaching/guides/guide-benchmarking.pdf
   - Benchmarking in Software Engineering by Alessandro Garcia 2013
     - http://www.inf.puc-rio.br/~inf2921/2013_2/docs/aulas/INF2921_aula6.pdf
   - "How not to lie with Statistics: The Correct Way to Summarize Benchmark Results", Philip J. Fleming and John J. Wallace 1986
     - http://ece.uprm.edu/~nayda/Courses/Icom5047F06/Papers/paper4.pdf
